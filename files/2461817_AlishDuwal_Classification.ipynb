{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Portfolio Project - Classification Task\n",
    "## Crop Prediction Based on Soil and Environmental Features\n",
    "\n",
    "**Student Name:** Alish Duwal  \n",
    "**Student ID:** 2461817  \n",
    "**Group:** L5CG2  \n",
    "\n",
    "**UN Sustainable Development Goal:** SDG 2 - Zero Hunger  \n",
    "**Objective:** Predict the type of crop to be grown based on soil nutrients and environmental conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# Classical ML Models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Neural Network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Feature Selection\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('Set2')\n",
    "\n",
    "print(\"✓ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('/content/drive/MyDrive/FinalAssessment/sensor_Crop_Dataset.csv', encoding='latin1')\n",
    "\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"First 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Information\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\"*50)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "print(\"=\"*50)\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "print(f\"\\nTotal missing values: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Summary\n",
    "print(\"Statistical Summary of Numerical Features:\")\n",
    "print(\"=\"*50)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check target variable distribution\n",
    "print(\"Target Variable (Crop) Distribution:\")\n",
    "print(\"=\"*50)\n",
    "print(df['Crop'].value_counts())\n",
    "print(f\"\\nNumber of unique crops: {df['Crop'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "### 3.1 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values if any\n",
    "print(\"Handling missing values...\")\n",
    "df_clean = df.dropna()\n",
    "print(f\"Rows after removing missing values: {len(df_clean)}\")\n",
    "print(f\"Rows removed: {len(df) - len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant features for crop prediction\n",
    "# We'll use environmental features: Nitrogen, Phosphorus, Potassium, Temperature, Humidity, pH_Value, Rainfall\n",
    "print(\"Selected Features for Crop Prediction:\")\n",
    "print(\"=\"*50)\n",
    "selected_features = ['Nitrogen', 'Phosphorus', 'Potassium', 'Temperature', 'Humidity', 'pH_Value', 'Rainfall']\n",
    "print(\"Features:\", selected_features)\n",
    "print(\"Target: Crop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop distribution visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "crop_counts = df_clean['Crop'].value_counts()\n",
    "plt.subplot(1, 2, 1)\n",
    "crop_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Crop Types', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Crop Type')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "crop_counts.plot(kind='pie', autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Crop Type Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Insight: The dataset shows distribution of different crop types.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of numerical features\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(selected_features):\n",
    "    axes[idx].hist(df_clean[col], bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(f'Distribution of {col}', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Hide extra subplots\n",
    "for idx in range(len(selected_features), 9):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Insight: Histograms show the distribution of environmental and soil features.\")\n",
    "print(\"Most features appear to have varied distributions across different ranges.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = df_clean[selected_features].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Heatmap of Environmental Features', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Insight: The correlation heatmap reveals relationships between features.\")\n",
    "print(\"Features with low correlation are more independent and valuable for prediction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot for outlier detection\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(selected_features):\n",
    "    axes[idx].boxplot(df_clean[col], vert=True, patch_artist=True,\n",
    "                     boxprops=dict(facecolor='lightblue', color='blue'),\n",
    "                     medianprops=dict(color='red', linewidth=2))\n",
    "    axes[idx].set_title(f'Boxplot of {col}', fontsize=10, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Value')\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Hide extra subplot\n",
    "axes[7].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Insight: Boxplots help identify outliers in the dataset.\")\n",
    "print(\"Some features may have extreme values that could affect model performance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features (X) and target (y)\n",
    "X = df_clean[selected_features]\n",
    "y = df_clean['Crop']\n",
    "\n",
    "print(\"Features (X) shape:\", X.shape)\n",
    "print(\"Target (y) shape:\", y.shape)\n",
    "print(\"\\nNumber of samples:\", len(X))\n",
    "print(\"Number of features:\", X.shape[1])\n",
    "print(\"Number of classes:\", y.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target variable\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "print(\"Target variable encoded successfully!\")\n",
    "print(\"Classes:\", le.classes_)\n",
    "print(\"\\nEncoded target shape:\", y_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(\"Data split successfully!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Training set size: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"Testing set size: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "print(\"\\nTraining features shape:\", X_train.shape)\n",
    "print(\"Testing features shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling (Important for Neural Networks and some algorithms)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Features scaled successfully using StandardScaler!\")\n",
    "print(\"\\nScaled training features shape:\", X_train_scaled.shape)\n",
    "print(\"Scaled testing features shape:\", X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Task 1: Build Neural Network Model (MLPClassifier)\n",
    "\n",
    "**Architecture:**\n",
    "- Input Layer: 7 features\n",
    "- Hidden Layer 1: 64 neurons with ReLU activation\n",
    "- Hidden Layer 2: 32 neurons with ReLU activation\n",
    "- Output Layer: Number of crop classes with Softmax (implicit)\n",
    "- Loss Function: Cross-Entropy Loss\n",
    "- Optimizer: Adam\n",
    "- Learning Rate: 0.001 (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Neural Network Classifier\n",
    "nn_classifier = MLPClassifier(\n",
    "    hidden_layer_sizes=(64, 32),  # Two hidden layers: 64 and 32 neurons\n",
    "    activation='relu',             # ReLU activation function\n",
    "    solver='adam',                 # Adam optimizer\n",
    "    learning_rate_init=0.001,      # Learning rate\n",
    "    max_iter=500,                  # Maximum iterations\n",
    "    random_state=42,\n",
    "    early_stopping=True,           # Use early stopping to prevent overfitting\n",
    "    validation_fraction=0.1,       # 10% of training data for validation\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"Neural Network Architecture:\")\n",
    "print(\"=\"*50)\n",
    "print(\"Input Layer: 7 features\")\n",
    "print(\"Hidden Layer 1: 64 neurons (ReLU activation)\")\n",
    "print(\"Hidden Layer 2: 32 neurons (ReLU activation)\")\n",
    "print(f\"Output Layer: {len(le.classes_)} neurons (Softmax activation)\")\n",
    "print(\"\\nOptimizer: Adam\")\n",
    "print(\"Loss Function: Cross-Entropy Loss\")\n",
    "print(\"Learning Rate: 0.001\")\n",
    "print(\"Max Iterations: 500\")\n",
    "print(\"Early Stopping: Enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Neural Network\n",
    "print(\"Training Neural Network...\")\n",
    "nn_classifier.fit(X_train_scaled, y_train)\n",
    "print(\"✓ Neural Network training completed!\")\n",
    "print(f\"\\nNumber of iterations: {nn_classifier.n_iter_}\")\n",
    "print(f\"Loss: {nn_classifier.loss_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Neural Network on Training Set\n",
    "y_train_pred_nn = nn_classifier.predict(X_train_scaled)\n",
    "\n",
    "print(\"Neural Network - Training Set Performance:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy: {accuracy_score(y_train, y_train_pred_nn):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_train, y_train_pred_nn, average='weighted'):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_train, y_train_pred_nn, average='weighted'):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_train, y_train_pred_nn, average='weighted'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Neural Network on Test Set\n",
    "y_test_pred_nn = nn_classifier.predict(X_test_scaled)\n",
    "\n",
    "print(\"Neural Network - Test Set Performance:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred_nn):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_test_pred_nn, average='weighted'):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_test_pred_nn, average='weighted'):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_test_pred_nn, average='weighted'):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Classification Report:\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(y_test, y_test_pred_nn, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for Neural Network\n",
    "cm_nn = confusion_matrix(y_test, y_test_pred_nn)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_nn, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title('Neural Network - Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Task 2: Build Two Classical ML Models\n",
    "\n",
    "### 6.1 Model 1: Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "print(\"Training Random Forest Classifier...\")\n",
    "rf_classifier.fit(X_train_scaled, y_train)\n",
    "print(\"✓ Random Forest training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Random Forest on Test Set\n",
    "y_test_pred_rf = rf_classifier.predict(X_test_scaled)\n",
    "\n",
    "print(\"Random Forest - Test Set Performance:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred_rf):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_test_pred_rf, average='weighted'):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_test_pred_rf, average='weighted'):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_test_pred_rf, average='weighted'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Model 2: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Logistic Regression Classifier\n",
    "lr_classifier = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "print(\"Training Logistic Regression Classifier...\")\n",
    "lr_classifier.fit(X_train_scaled, y_train)\n",
    "print(\"✓ Logistic Regression training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Logistic Regression on Test Set\n",
    "y_test_pred_lr = lr_classifier.predict(X_test_scaled)\n",
    "\n",
    "print(\"Logistic Regression - Test Set Performance:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred_lr):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_test_pred_lr, average='weighted'):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_test_pred_lr, average='weighted'):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_test_pred_lr, average='weighted'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Task 3: Hyperparameter Optimization with Cross-Validation\n",
    "\n",
    "### 7.1 Random Forest Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for Random Forest (reduced for faster execution)\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "print(\"Random Forest - Hyperparameter Grid:\")\n",
    "print(\"=\"*50)\n",
    "for param, values in rf_param_grid.items():\n",
    "    print(f\"{param}: {values}\")\n",
    "\n",
    "print(f\"\\nTotal combinations: {np.prod([len(v) for v in rf_param_grid.values()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform GridSearchCV for Random Forest\n",
    "print(\"\\nPerforming GridSearchCV for Random Forest...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "rf_grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    rf_param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\n✓ GridSearchCV completed for Random Forest!\")\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(\"=\"*50)\n",
    "for param, value in rf_grid_search.best_params_.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "print(f\"\\nBest Cross-Validation Score: {rf_grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Logistic Regression Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for Logistic Regression (reduced for faster execution)\n",
    "lr_param_grid = {\n",
    "    'C': [0.1, 1.0, 10.0],\n",
    "    'solver': ['lbfgs', 'liblinear'],\n",
    "    'penalty': ['l2']\n",
    "}\n",
    "\n",
    "print(\"Logistic Regression - Hyperparameter Grid:\")\n",
    "print(\"=\"*50)\n",
    "for param, values in lr_param_grid.items():\n",
    "    print(f\"{param}: {values}\")\n",
    "\n",
    "print(f\"\\nTotal combinations: {np.prod([len(v) for v in lr_param_grid.values()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform GridSearchCV for Logistic Regression\n",
    "print(\"\\nPerforming GridSearchCV for Logistic Regression...\")\n",
    "\n",
    "lr_grid_search = GridSearchCV(\n",
    "    LogisticRegression(max_iter=1000, random_state=42),\n",
    "    lr_param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "lr_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\n✓ GridSearchCV completed for Logistic Regression!\")\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(\"=\"*50)\n",
    "for param, value in lr_grid_search.best_params_.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "print(f\"\\nBest Cross-Validation Score: {lr_grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Task 4: Feature Selection\n",
    "\n",
    "We will use SelectKBest with f_classif for feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection using SelectKBest\n",
    "k_best = 5  # Select top 5 features\n",
    "\n",
    "selector = SelectKBest(score_func=f_classif, k=k_best)\n",
    "X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
    "X_test_selected = selector.transform(X_test_scaled)\n",
    "\n",
    "# Get selected feature names\n",
    "selected_feature_indices = selector.get_support(indices=True)\n",
    "selected_feature_names = [selected_features[i] for i in selected_feature_indices]\n",
    "\n",
    "print(\"Feature Selection Results:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Method: SelectKBest with f_classif\")\n",
    "print(f\"Number of features selected: {k_best}\")\n",
    "print(f\"\\nSelected Features: {selected_feature_names}\")\n",
    "print(f\"\\nFeature Scores:\")\n",
    "for i, (feature, score) in enumerate(zip(selected_features, selector.scores_)):\n",
    "    selected = \"✓\" if i in selected_feature_indices else \"✗\"\n",
    "    print(f\"{selected} {feature}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "feature_scores = pd.DataFrame({\n",
    "    'Feature': selected_features,\n",
    "    'Score': selector.scores_\n",
    "}).sort_values('Score', ascending=False)\n",
    "\n",
    "colors = ['green' if f in selected_feature_names else 'lightgray' for f in feature_scores['Feature']]\n",
    "\n",
    "plt.barh(feature_scores['Feature'], feature_scores['Score'], color=colors, edgecolor='black')\n",
    "plt.xlabel('F-Score', fontsize=12)\n",
    "plt.ylabel('Features', fontsize=12)\n",
    "plt.title('Feature Importance Scores (SelectKBest)', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGreen bars indicate selected features for final models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Task 5: Final Models with Optimal Hyperparameters and Selected Features\n",
    "\n",
    "### 9.1 Final Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build final Random Forest model with best parameters and selected features\n",
    "final_rf = RandomForestClassifier(**rf_grid_search.best_params_, random_state=42)\n",
    "\n",
    "print(\"Training Final Random Forest Model...\")\n",
    "print(\"=\"*50)\n",
    "print(\"Features used:\", selected_feature_names)\n",
    "print(\"Number of features:\", len(selected_feature_names))\n",
    "print(\"\\nHyperparameters:\")\n",
    "for param, value in rf_grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "final_rf.fit(X_train_selected, y_train)\n",
    "print(\"\\n✓ Final Random Forest model trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate final Random Forest model\n",
    "y_test_pred_rf_final = final_rf.predict(X_test_selected)\n",
    "\n",
    "# Get cross-validation score\n",
    "rf_cv_score = cross_val_score(final_rf, X_train_selected, y_train, cv=5, scoring='accuracy').mean()\n",
    "\n",
    "rf_final_accuracy = accuracy_score(y_test, y_test_pred_rf_final)\n",
    "rf_final_precision = precision_score(y_test, y_test_pred_rf_final, average='weighted')\n",
    "rf_final_recall = recall_score(y_test, y_test_pred_rf_final, average='weighted')\n",
    "rf_final_f1 = f1_score(y_test, y_test_pred_rf_final, average='weighted')\n",
    "\n",
    "print(\"Final Random Forest - Test Set Performance:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"CV Score: {rf_cv_score:.4f}\")\n",
    "print(f\"Accuracy: {rf_final_accuracy:.4f}\")\n",
    "print(f\"Precision: {rf_final_precision:.4f}\")\n",
    "print(f\"Recall: {rf_final_recall:.4f}\")\n",
    "print(f\"F1-Score: {rf_final_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Final Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build final Logistic Regression model with best parameters and selected features\n",
    "final_lr = LogisticRegression(**lr_grid_search.best_params_, max_iter=1000, random_state=42)\n",
    "\n",
    "print(\"Training Final Logistic Regression Model...\")\n",
    "print(\"=\"*50)\n",
    "print(\"Features used:\", selected_feature_names)\n",
    "print(\"Number of features:\", len(selected_feature_names))\n",
    "print(\"\\nHyperparameters:\")\n",
    "for param, value in lr_grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "final_lr.fit(X_train_selected, y_train)\n",
    "print(\"\\n✓ Final Logistic Regression model trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate final Logistic Regression model\n",
    "y_test_pred_lr_final = final_lr.predict(X_test_selected)\n",
    "\n",
    "# Get cross-validation score\n",
    "lr_cv_score = cross_val_score(final_lr, X_train_selected, y_train, cv=5, scoring='accuracy').mean()\n",
    "\n",
    "lr_final_accuracy = accuracy_score(y_test, y_test_pred_lr_final)\n",
    "lr_final_precision = precision_score(y_test, y_test_pred_lr_final, average='weighted')\n",
    "lr_final_recall = recall_score(y_test, y_test_pred_lr_final, average='weighted')\n",
    "lr_final_f1 = f1_score(y_test, y_test_pred_lr_final, average='weighted')\n",
    "\n",
    "print(\"Final Logistic Regression - Test Set Performance:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"CV Score: {lr_cv_score:.4f}\")\n",
    "print(f\"Accuracy: {lr_final_accuracy:.4f}\")\n",
    "print(f\"Precision: {lr_final_precision:.4f}\")\n",
    "print(f\"Recall: {lr_final_recall:.4f}\")\n",
    "print(f\"F1-Score: {lr_final_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Task 6: Final Model Comparison\n",
    "\n",
    "Comparison of all models including Neural Network and optimized classical models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison table\n",
    "comparison_data = {\n",
    "    'Model': [\n",
    "        'Neural Network (MLP)',\n",
    "        'Random Forest (Optimized)',\n",
    "        'Logistic Regression (Optimized)'\n",
    "    ],\n",
    "    'Features Used': [\n",
    "        f'All ({len(selected_features)})',\n",
    "        f'Selected ({len(selected_feature_names)})',\n",
    "        f'Selected ({len(selected_feature_names)})'\n",
    "    ],\n",
    "    'CV Score': [\n",
    "        'N/A',\n",
    "        f'{rf_cv_score:.4f}',\n",
    "        f'{lr_cv_score:.4f}'\n",
    "    ],\n",
    "    'Accuracy': [\n",
    "        f'{accuracy_score(y_test, y_test_pred_nn):.4f}',\n",
    "        f'{rf_final_accuracy:.4f}',\n",
    "        f'{lr_final_accuracy:.4f}'\n",
    "    ],\n",
    "    'Precision': [\n",
    "        f'{precision_score(y_test, y_test_pred_nn, average=\"weighted\"):.4f}',\n",
    "        f'{rf_final_precision:.4f}',\n",
    "        f'{lr_final_precision:.4f}'\n",
    "    ],\n",
    "    'Recall': [\n",
    "        f'{recall_score(y_test, y_test_pred_nn, average=\"weighted\"):.4f}',\n",
    "        f'{rf_final_recall:.4f}',\n",
    "        f'{lr_final_recall:.4f}'\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        f'{f1_score(y_test, y_test_pred_nn, average=\"weighted\"):.4f}',\n",
    "        f'{rf_final_f1:.4f}',\n",
    "        f'{lr_final_f1:.4f}'\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL MODEL COMPARISON TABLE\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "nn_scores = [\n",
    "    accuracy_score(y_test, y_test_pred_nn),\n",
    "    precision_score(y_test, y_test_pred_nn, average='weighted'),\n",
    "    recall_score(y_test, y_test_pred_nn, average='weighted'),\n",
    "    f1_score(y_test, y_test_pred_nn, average='weighted')\n",
    "]\n",
    "rf_scores = [rf_final_accuracy, rf_final_precision, rf_final_recall, rf_final_f1]\n",
    "lr_scores = [lr_final_accuracy, lr_final_precision, lr_final_recall, lr_final_f1]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars1 = ax.bar(x - width, nn_scores, width, label='Neural Network', color='steelblue')\n",
    "bars2 = ax.bar(x, rf_scores, width, label='Random Forest', color='forestgreen')\n",
    "bars3 = ax.bar(x + width, lr_scores, width, label='Logistic Regression', color='coral')\n",
    "\n",
    "ax.set_xlabel('Metrics', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Final Model Performance Comparison', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim([0, 1.1])\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2, bars3]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusion and Reflection\n",
    "\n",
    "### Model Performance Summary\n",
    "\n",
    "All three models achieved high performance in predicting crop types based on environmental and soil features. The Neural Network, Random Forest, and Logistic Regression models all demonstrated strong accuracy, precision, recall, and F1-scores on the test set.\n",
    "\n",
    "### Impact of Methods\n",
    "\n",
    "**Cross-Validation:** GridSearchCV helped identify optimal hyperparameters for both Random Forest and Logistic Regression, potentially improving their generalization performance.\n",
    "\n",
    "**Feature Selection:** Using SelectKBest, we reduced the feature space from 7 to 5 features, which:\n",
    "- Simplified the models\n",
    "- Reduced computational complexity\n",
    "- Maintained or improved model performance\n",
    "- Identified the most important environmental factors for crop prediction\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **Environmental features** (Nitrogen, Phosphorus, Potassium, Temperature, Humidity, pH, Rainfall) are strong predictors of suitable crop types\n",
    "2. **Feature selection** successfully identified the most discriminative features while reducing dimensionality\n",
    "3. Both **classical ML models** and **Neural Networks** performed well on this classification task\n",
    "4. The models can help farmers make **data-driven decisions** about crop selection, supporting **SDG 2: Zero Hunger**\n",
    "\n",
    "### Future Directions\n",
    "\n",
    "1. Experiment with **ensemble methods** combining multiple models\n",
    "2. Collect **more diverse data** across different geographical regions\n",
    "3. Incorporate **additional features** like soil texture, elevation, and climate patterns\n",
    "4. Deploy the model as a **web application** for real-world agricultural use\n",
    "5. Investigate **model interpretability** to understand feature contributions better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLASSIFICATION TASK COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nAll required tasks have been completed:\")\n",
    "print(\"✓ Task 1: Exploratory Data Analysis\")\n",
    "print(\"✓ Task 2: Neural Network Model (MLPClassifier)\")\n",
    "print(\"✓ Task 3: Two Classical ML Models (Random Forest & Logistic Regression)\")\n",
    "print(\"✓ Task 4: Hyperparameter Optimization with Cross-Validation\")\n",
    "print(\"✓ Task 5: Feature Selection (SelectKBest)\")\n",
    "print(\"✓ Task 6: Final Model Comparison\")\n",
    "print(\"✓ Task 7: Conclusion and Reflection\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
